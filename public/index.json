
[{"content":" ","date":"12 October 2024","externalUrl":null,"permalink":"/blog/","section":"Blog","summary":" ","title":"Blog","type":"blog"},{"content":"\rOverview #\rBayes\u0026rsquo; Theorem is a fundamental concept in mathematics. But, did you ever know how it is used in real-life?\n","date":"12 October 2024","externalUrl":null,"permalink":"/blog/naive-bayes/","section":"Blog","summary":"Overview #\rBayes\u0026rsquo; Theorem is a fundamental concept in mathematics.","title":"How Bayes' Theorem is used in Machine Learning","type":"blog"},{"content":"","date":"12 October 2024","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"","date":"12 October 2024","externalUrl":null,"permalink":"/","section":"nveshaan","summary":"","title":"nveshaan","type":"page"},{"content":"","date":"12 October 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"\rOverview #\rIn the previous post, I trained a neural network to classify handwritten digits, achieving an accuracy of approximately 85% on the test data. The training process took more than an hour. To expedite the training process, I will now vectorize the implementation.\nImplementation #\rInstead of iterating through the training examples one by one, I will stack them up in a matrixÂ XÂ and one-hot encode the labels inÂ Y. Then I can multiplyÂ XÂ withÂ WÂ and addÂ BÂ to get the outputÂ y.\nBefore vectorising, I multiplied the weights with the training examples and added the bias values in the following way\n$$ { \\begin{bmatrix} w_{11} \u0026amp; w_{12} \u0026amp; w_{13} \u0026amp; w_{14}\\newline w_{21} \u0026amp; w_{22} \u0026amp; w_{23} \u0026amp; w_{24}\\newline w_{31} \u0026amp; w_{32} \u0026amp; w_{33} \u0026amp; w_{34} \\end{bmatrix} \\begin{bmatrix} x_{11}\\newline x_{12}\\newline x_{13}\\newline x_{14} \\end{bmatrix} + \\begin{bmatrix} b_1\\newline b_2\\newline b_3 \\end{bmatrix} } $$\n$$ { = \\begin{bmatrix} w_{11}x_{11}+w_{12}x_{12}+w_{13}x_{13}+w_{14}x_{14}+b_1\\newline w_{21}x_{11}+w_{22}x_{12}+w_{23}x_{13}+w_{24}x_{14}+b_2\\newline w_{31}x_{11}+w_{32}x_{12}+w_{33}x_{13}+w_{34}x_{14}+b_3 \\end{bmatrix} = \\begin{bmatrix} a_1\\newline a_2\\newline a_3 \\end{bmatrix} } $$\nNow, I can multiply all the training examples with the weights and add the bias values in one step\n$$ { \\begin{bmatrix} w_{11} \u0026amp; w_{12} \u0026amp; w_{13} \u0026amp; w_{14}\\newline w_{21} \u0026amp; w_{22} \u0026amp; w_{23} \u0026amp; w_{24}\\newline w_{31} \u0026amp; w_{32} \u0026amp; w_{33} \u0026amp; w_{34} \\end{bmatrix} \\begin{bmatrix} x_{11} \u0026amp; x_{21} \u0026amp; x_{31} \u0026amp; x_{41}\\newline x_{12} \u0026amp; x_{22} \u0026amp; x_{32} \u0026amp; x_{42}\\newline x_{13} \u0026amp; x_{23} \u0026amp; x_{33} \u0026amp; x_{43}\\newline x_{14} \u0026amp; x_{24} \u0026amp; x_{34} \u0026amp; x_{44} \\end{bmatrix} + \\begin{bmatrix} b_1\\newline b_2\\newline b_3 \\end{bmatrix} } $$\n$$ { = \\begin{bmatrix} a_{11} \u0026amp; a_{21} \u0026amp; a_{31} \u0026amp; a_{41}\\newline a_{12} \u0026amp; a_{22} \u0026amp; a_{32} \u0026amp; a_{42}\\newline a_{13} \u0026amp; a_{23} \u0026amp; a_{33} \u0026amp; a_{43} \\end{bmatrix} } $$\nIn the code, I got rid of one for loop.\nfor run in range(epoch): # forward prop Z1 = np.dot(W1, X) + B1 A1 = 1/(1+np.exp(-Z1+1e-5)) Z2 = np.dot(W2, A1) + B2 A2 = 1/(1+np.exp(-Z2+1e-5)) Z3 = np.dot(W3, A2) + B3 y = np.exp(Z3+1e-5) y /= sum(y) # back prop dz3 = y - Y_one dw3 = np.dot(dz3, A2.T)/m db3 = sum(dz3.T)/m da2 = np.matmul(W3.T, dz3) dz2 = A2*(1-A2)*da2 dw2 = np.dot(dz2, A1.T)/m db2 = sum(dz2.T)/m da1 = np.matmul(W2.T, dz2) dz1 = A1*(1-A1)*da1 dw1 = np.dot(dz1, X.T)/m db1 = sum(dz1.T)/m # update params W3 -= alpha*dw3 B3 -= alpha*db3.reshape(-1, 1) W2 -= alpha*dw2 B2 -= alpha*db2.reshape(-1, 1) W1 -= alpha*dw1 B1 -= alpha*db1.reshape(-1, 1) Conclusion #\rThe training process became 15x faster in the vectorised form, producing similar results to the non-vectorised form.\n","date":"30 July 2024","externalUrl":null,"permalink":"/blog/vectorised-digit-classifier/","section":"Blog","summary":"Overview #\rIn the previous post, I trained a neural network to classify handwritten digits, achieving an accuracy of approximately 85% on the test data.","title":"Vectorising the Digit Classifier","type":"blog"},{"content":"\rOverview #\rIn this post, Iâ€™ll demonstrate how to build a neural network from scratch without relying on popular ML frameworks like PyTorch, TensorFlow, or Keras. Instead, Iâ€™ll use Python libraries such as numpy, pandas, and matplotlib to develop a model that classifies handwritten digits.\nWhy implement a Neural Net from scratch? #\rPlenty of ML frameworks offer out-of-the-box functionality for building neural networks. However, implementing one from scratch is a valuable exercise. It helps you understand how neural networks work under the hood, which is essential for designing effective models.\nFor a deep dive into neural networks, check out 3Blue1Brown\u0026rsquo;s series. Here, I\u0026rsquo;ll focus on the practical implementation.\rArchitecture #\rThe model will consist of an input layer of 784 neurons, two hidden layers with 16 neurons each and an output layer with 10 neurons. This is a very simple configuration relative to modern standards.\nBoth hidden layers use sigmoid as the activation function. The final layer goes through a softmax function. The cost function is categorical cross entropy.\nThe model uses batch gradient descent algorithm to find the minima of cost function.\nImplementation #\rSetup #\rAs mentioned above, I will import the required Python libraries.\nimport numpy as np import pandas as pd import matplotlib as plt Then, I will split the data into train and test sets, taking m = 30000 as number of training examples.\ndata = pd.read_csv(\u0026#39;/data.csv\u0026#39;) train = data[:30000] test = data[30000:] X = train.drop(columns=[\u0026#39;label\u0026#39;]).transpose() # input Y = train[\u0026#39;label\u0026#39;] # output X_t = test.drop(columns=[\u0026#39;label\u0026#39;]).transpose() # input Y_t = test[\u0026#39;label\u0026#39;] # output Now, I will one-hot encode the labels\nY_one = np.zeros((m, 10)) for i in range(m): Y_one[i][Y[i]] = 1 Y_one = Y_one.T and initialise weights and biases. np.random.rand() generates random values in [0, 1], I\u0026rsquo;ll subtract them with 0.5 for better performance\nW1 = np.random.rand(16, 784) - 0.5 # 16x784 matrix B1 = np.random.rand(16) - 0.5 # 16x1 matrix W2 = np.random.rand(16, 16) - 0.5 # 16x16 matrix B2 = np.random.rand(16) - 0.5 # 16x1 matrix W3 = np.random.rand(10, 16) - 0.5 # 10X16 matrix B3 = np.random.rand(10) - 0.5 # 10x1 matrix At first, these parameters are just random numbers. When used, they produce garbage results. As the model learns to predict the correct values, it tunes them to reasonable numbers. Which, when used, will yield good results.\nTraining #\rBefore going into the training process, I will discuss each part separately.\nForward Prop #\rI will feed the training examples to the input layers, multiplying them by the weights and adding the bias values. This output is then input into the first hidden layer, and this process continues to the final output layer.\nZ1 = np.dot(W1, X[i]) + B1 A1 = 1/(1+np.exp(-Z1+1e-5)) # sigmoid Z2 = np.dot(W2, A1) + B2 A2 = 1/(1+np.exp(-Z2+1e-5)) # sigmoid Z3 = np.dot(W3, A2) + B3 y = np.exp(Z3+1e-5) y /= sum(y) # softmax # I added a small value of 10^-5 to prevent the exponent function from vanishing Back Prop #\rBy applying the chain rule of calculus, I will compute the partial derivatives for each node in the layers. However, numpy makes implementing backpropagation much simpler.\ndz3 = y - Y_one.T[i] dw3 += np.outer(dz3, A2) db3 += dz3 da2 = np.dot(W3.T, dz3) dz2 = A2*(1 - A2)*da2 dw2 += np.outer(dz2, A1) db2 += dz2 da1 = np.dot(W2.T, dz2) dz1 = A1*(1 - A1)*da1 dw1 += np.outer(dz1, X[i]) db1 += dz1 I\u0026rsquo;m summing all the increments and decrements to the weights and biases across all training examples and updating them at the end of the epoch.\nUpdate Params #\rI\u0026rsquo;ll adjust the parameters once I\u0026rsquo;ve gone through all the training examples.\nW3 -= alpha*dw3/m B3 -= alpha*db3/m W2 -= alpha*dw2/m B2 -= alpha*db2/m W1 -= alpha*dw1/m B1 -= alpha*db1/m I will repeat the process through multiple iterations (or epochs) to find the minimum value.\nPutting it all together #\rfor run in range(epoch): dw1 = np.zeros((16, 784)) db1 = np.zeros(16) dw2 = np.zeros((16, 16)) db2 = np.zeros(16) dw3 = np.zeros((10, 16)) db3 = np.zeros(10) for i in range(m): # Forward prop Z1 = np.dot(W1, X[i]) + B1 A1 = 1/(1+np.exp(-Z1+1e-5)) Z2 = np.dot(W2, A1) + B2 A2 = 1/(1+np.exp(-Z2+1e-5)) Z3 = np.dot(W3, A2) + B3 y = np.exp(Z3+1e-5) y /= sum(y) # Back prop dz3 = y - Y_one.T[i] dw3 += np.outer(dz3, A2) db3 += dz3 da2 = np.dot(W3.T, dz3) dz2 = A2*(1 - A2)*da2 dw2 += np.outer(dz2, A1) db2 += dz2 da1 = np.dot(W2.T, dz2) dz1 = A1*(1 - A1)*da1 dw1 += np.outer(dz1, X[i]) db1 += dz1 # Update params W3 -= alpha*dw3/m B3 -= alpha*db3/m W2 -= alpha*dw2/m B2 -= alpha*db2/m W1 -= alpha*dw1/m B1 -= alpha*db1/m Results #\rI obtained the following results after training the model for epoch = 500 with a learning rate alpha = 0.8.\nTraining accuracy: 87.22 %\nTest accuracy: 85.65 %\nThe model generalises the test data very well. The cost v/s epoch plot shows that the cost function converged to a value.\nI executed the code in Google Colab. The link to that notebook is here.\n","date":"21 July 2024","externalUrl":null,"permalink":"/blog/digit-classifier/","section":"Blog","summary":"Overview #\rIn this post, Iâ€™ll demonstrate how to build a neural network from scratch without relying on popular ML frameworks like PyTorch, TensorFlow, or Keras.","title":"Coding a Neural Network from scratch","type":"blog"},{"content":"HeyðŸ‘‹, Iâ€™m an undergraduate at IISER Thiruvananthapuram, studying Mathematics, Physics, and Data Science. I created this website to share my knowledge and experiences, hoping it will serve as a platform to showcase my academic projects, coding endeavours, and insights gained during my studies.\nOverview My school exposed me to programming at the age of 13. I started coding in Java and wrote several programs as part of my assignments. One of my early projects was a program that translated Morse code into English, which impressed my teacher. My friends and I then collaborated on another program that translated our secret code language.\nWhen the pandemic hit, I was locked down at home and took the opportunity to expand my skills further. I learned how to create websites (React), explored Python, and discovered Manim, a library for animating mathematical concepts.\nOver the years, I have explored various fields, but my primary interest lies in, Currently, I am delving into artificial intelligence, working on small projects to apply and expand my knowledge in this exciting area.\nHobbies Aside from my academic pursuits, I enjoy playing badminton. I love listening to Shreya Ghoshal\u0026rsquo;s songs (also Cherry\u0026rsquo;s songs). I\u0026rsquo;m passionate about cooking and eating, and I enjoy travelling and doing adventure activities.\n","externalUrl":null,"permalink":"/about/","section":"nveshaan","summary":"HeyðŸ‘‹, Iâ€™m an undergraduate at IISER Thiruvananthapuram, studying Mathematics, Physics, and Data Science.","title":"About","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"\rOverview #\rThe project is a simple navigater that can navigate through a maze without human intervention. It has been designed to be a simple, easy-to-use navigater for navigating through a maze.\n","externalUrl":null,"permalink":"/projects/autonomous-navigater/","section":"Projects","summary":"Overview #\rThe project is a simple navigater that can navigate through a maze without human intervention.","title":"Autonomous Navigater","type":"projects"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"\rOverview #\rMinimax is a popular algorithm used in the game of chess. It is also used in many other games such as checkers, checkers, checkers, and checkers. Minimax is a fundamental concept in the game of chess.\n","externalUrl":null,"permalink":"/projects/chess-engine/","section":"Projects","summary":"Overview #\rMinimax is a popular algorithm used in the game of chess.","title":"Chess Engine","type":"projects"},{"content":"Feel free to connect with me:\nGitHub LinkedIn Email (nveshaan23@iisertvm.ac.in) CV (PDF) ","externalUrl":null,"permalink":"/contact/","section":"nveshaan","summary":"Feel free to connect with me:","title":"Contact","type":"page"},{"content":" Overview #\rElectromyography (EMG) measures the electrical activity generated by skeletal muscles during contractions, typically using surface electrodes. This technique enables the identification of specific gestures by analyzing the signals, making it particularly useful for applications in human-computer interaction. By interpreting muscle activity, devices can effectively respond to user intentions, thereby enhancing assistive technologies for individuals with mobility impairments.\nTo achieve high-accuracy human-computer interaction while minimizing the calibration data required from new users, a neural network is trained to classify gestures from EMG signals. This approach ensures seamless integration, allowing devices to adapt quickly to individual users and provide a more intuitive and responsive experience.\nThe project is divided into three main parts:\nData Collection Model Design and Training Real Time Interface Data Acquisition #\rThe dataset for this project has been collected from 13 subjects, using an EMG sensor from UpsideDownLabs.\n","externalUrl":null,"permalink":"/projects/muscle-wave-classifier/","section":"Projects","summary":"Overview #\rElectromyography (EMG) measures the electrical activity generated by skeletal muscles during contractions, typically using surface electrodes.","title":"Muscle Wave Classifier","type":"projects"},{"content":" ","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":" ","title":"Projects","type":"projects"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]