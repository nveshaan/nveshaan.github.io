
[{"content":" ","date":"21 July 2024","externalUrl":null,"permalink":"/blog/","section":"Blog","summary":" ","title":"Blog","type":"blog"},{"content":"\rOverview #\rIn this post, I will implement a neural network without using popular ML frameworks like PyTorch, TensorFlow, Keras. I will use Python libraries like numpy, pandas, matplotlib to create a model that classifies hand-written digits.\nWhy implement a Neural Net from scratch? #\rThere are plenty of ML frameworks that offer out-of-the-box functionality for building neural networks. However, implementing one from scratch is a valuable exercise. It helps you understand how neural networks work under the hood, which is essential for designing effective models.\nFor a deep dive into neural networks, check out 3Blue1Brown\u0026rsquo;s series. Here, I\u0026rsquo;ll focus on the practical implementation.\rArchitecture #\rThe model will consist of an input layer of 784 neurons, two hidden layers with 16 neurons each and an output layer with 10 neurons. This is a very simple configuration relative to modern standards.\nBoth hidden layers use sigmoid as the activation functions. The final layer goes through a softmax function.\nThe model uses batch gradient descent algorithm to find minima of cost function.\nImplementation #\rSetup #\rAs mentioned above, I will import the required Python libraries.\nimport numpy as np import pandas as pd import matplotlib as plt Then, I will split the data into train and test sets, taking m = 30000 as number of traning examples.\ndata = pd.read_csv(\u0026#39;/data.csv\u0026#39;) train = data[:30000] test = data[30000:] X = train.drop(columns=[\u0026#39;label\u0026#39;]).transpose() # input Y = train[\u0026#39;label\u0026#39;] # output X_t = test.drop(columns=[\u0026#39;label\u0026#39;]).transpose() # input Y_t = test[\u0026#39;label\u0026#39;] # output Now, I will one-hot encode the labels\nY_one = np.zeros((m, 10)) for i in range(m): Y_one[i][Y[i]] = 1 Y_one = Y_one.T and initialise weights and biases.\nW1 = np.random.rand(16, 784) # 16x784 matrix B1 = np.random.rand(16) # 16x1 matrix W2 = np.random.rand(16, 16) # 16x16 matrix B2 = np.random.rand(16) # 16x1 matrix W3 = np.random.rand(10, 16) # 10X16 matrix B3 = np.random.rand(10) # 10x1 matrix At first, these parameters are just random numbers. When used, they produce garbage results. As the model learns to predict the correct values, it tunes them to reasonable numbers. Which, when used, will yield good results.\nTraining #\rBefore going into the training process, I will discuss each part separately.\nForward Prop #\rI will feed the training examples to the input layers, multiplying them by the weights and adding the bias values. This output is then input into the first hidden layer, and this process continues to the final output layer.\nZ1 = np.dot(W1, X[i]) + B1 A1 = 1/(1+np.exp(-Z1+1e-5)) # sigmoid Z2 = np.dot(W2, A1) + B2 A2 = 1/(1+np.exp(-Z2+1e-5)) # sigmoid Z3 = np.dot(W3, A2) + B3 y = np.exp(Z3+1e-5) y /= sum(y) # softmax # I added a small value of 10^-5 to prevent the exponent function to vanish Back Prop #\rUsing the chain rule of calculus, I will calculate respective partial derivatives of each node in the layers. But, thanks to numpy, it is a lot easier to implement the backprop.\ndz3 = y - Y_one.T[i] dw3 += np.outer(dz3, A2) db3 += dz3 da2 = np.dot(W3.T, dz3) dz2 = A2*(1 - A2)*da2 dw2 += np.outer(dz2, A1) db2 += dz2 da1 = np.dot(W2.T, dz2) dz1 = A1*(1 - A1)*da1 dw1 += np.outer(dz1, X[i]) db1 += dz1 I\u0026rsquo;m adding up all the increments and decrements to the weights and biases, for all the training examples and update them at the end of epoch.\nUpdate Params #\rAfter iterating through all the training examples, I\u0026rsquo;ll update the parameters.\nW3 -= alpha*dw3/m B3 -= alpha*db3/m W2 -= alpha*dw2/m B2 -= alpha*db2/m W1 -= alpha*dw1/m B1 -= alpha*db1/m This process will repeat for several iterations (or epochs) to reach the minima.\nPutting it all together #\rfor run in range(epoch): dw1 = np.zeros((16, 784)) db1 = np.zeros(16) dw2 = np.zeros((16, 16)) db2 = np.zeros(16) dw3 = np.zeros((10, 16)) db3 = np.zeros(10) for i in range(m): # Forward prop Z1 = np.dot(W1, X[i]) + B1 A1 = 1/(1+np.exp(-Z1+1e-5)) Z2 = np.dot(W2, A1) + B2 A2 = 1/(1+np.exp(-Z2+1e-5)) Z3 = np.dot(W3, A2) + B3 y = np.exp(Z3+1e-5) y /= sum(y) # Back prop dz3 = y - Y_one.T[i] dw3 += np.outer(dz3, A2) db3 += dz3 da2 = np.dot(W3.T, dz3) dz2 = A2*(1 - A2)*da2 dw2 += np.outer(dz2, A1) db2 += dz2 da1 = np.dot(W2.T, dz2) dz1 = A1*(1 - A1)*da1 dw1 += np.outer(dz1, X[i]) db1 += dz1 # Update params W3 -= alpha*dw3/m B3 -= alpha*db3/m W2 -= alpha*dw2/m B2 -= alpha*db2/m W1 -= alpha*dw1/m B1 -= alpha*db1/m Results #\r","date":"21 July 2024","externalUrl":null,"permalink":"/blog/digit-classifier/","section":"Blog","summary":"Overview #\rIn this post, I will implement a neural network without using popular ML frameworks like PyTorch, TensorFlow, Keras.","title":"Coding a Neural Network from scratch","type":"blog"},{"content":"","date":"21 July 2024","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"","date":"21 July 2024","externalUrl":null,"permalink":"/","section":"nveshaan","summary":"","title":"nveshaan","type":"page"},{"content":"","date":"21 July 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"HeyðŸ‘‹, Iâ€™m an undergraduate at IISER Thiruvananthapuram, studying Mathematics, Physics, and Data Science. I created this website to share my knowledge and experiences, hoping it will serve as a platform to showcase my academic projects, coding endeavours, and insights gained during my studies.\nOverview My school exposed me to programming at the age of 13. I started coding in Java and wrote several programs as part of my assignments. One of my early projects was a program that translated Morse code into English, which impressed my teacher. My friends and I then collaborated on another program that translated our secret code language.\nWhen the pandemic hit, I was locked down at home and took the opportunity to expand my skills further. I learned how to create websites (React), explored Python, and discovered Manim, a library for animating mathematical concepts.\nOver the years, I have explored various fields, but my primary interest lies in, Currently, I am delving into artificial intelligence, working on small projects to apply and expand my knowledge in this exciting area.\nHobbies Aside from my academic pursuits, I enjoy playing tennis and badminton. I love listening to Shreya Ghoshal\u0026rsquo;s songs (also Cherry\u0026rsquo;s songs). I\u0026rsquo;m passionate about cooking and eating, and I enjoy travelling and doing adventure activities.\n","externalUrl":null,"permalink":"/about/","section":"nveshaan","summary":"HeyðŸ‘‹, Iâ€™m an undergraduate at IISER Thiruvananthapuram, studying Mathematics, Physics, and Data Science.","title":"About","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"Feel free to connect with me:\nGitHub LinkedIn Email (nveshaan23@iisertvm.ac.in) CV (PDF) ","externalUrl":null,"permalink":"/contact/","section":"nveshaan","summary":"Feel free to connect with me:","title":"Contact","type":"page"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]