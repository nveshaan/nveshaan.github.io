
[{"content":"\rOverview #\rBayes\u0026rsquo; Theorem is a fundamental concept in Probability Theory. It is widely used in fields such as statistics, machine learning, and data science, especially in the context of probabilistic inference and decision-making. In this post, I will explain how Bayes\u0026rsquo; Theorem is used in Machine Learning, by considering a simple example. But, before that, let\u0026rsquo;s understand some terminology.\nTerminology #\rBayes\u0026rsquo; Theorem #\rIt describes how to update the probability of a hypothesis based on new evidence. It provides a way to calculate the posterior probability of an event by combining the prior probability with the likelihood.\nMathematically, Bayes\u0026rsquo; Theorem is defined as:\n$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\nLikelihood vs. Probability #\rProbability measures how likely a particular event is to occur, given a certain model or assumption. In Bayes\u0026rsquo; Theorem, \\( P(B) \\) represents the total probability of observing event \\(B\\).\nLikelihood measures how likely the observed data is given a particular hypothesis or model. It\u0026rsquo;s similar to probability with a subtle difference. Probability is about predicting outcomes, while likelihood is about fitting a model to data. In Bayes\u0026rsquo; Theorem, \\( P(B|A)\\) is the likelihood of observing data \\(B\\) given the hypothesis \\(A\\).\nPrior Probability #\rThe prior probability represents your initial belief about the probability of a hypothesis before you have any new data or evidence. In Bayes\u0026rsquo; Theorem, \\( P(A) \\) is the prior probability of the hypothesis \\(A\\).\nPosterior Probability #\rThe posterior probability is the updates probability of a hypothesis after observing new evidence. It combines the prior belief with the likelihood of the observed data. In Bayes\u0026rsquo; Theorem, \\( P(A|B) \\) is the posterior probability of the hypothesis \\(A\\) given the evidence \\(B\\).\nImplementation #\rOne of the use-cases of Naive Bayes\u0026rsquo; Classifier is filtering out spam mails from your inbox. Let\u0026rsquo;s see how it is implemented.\nFor a classifier, we need to define three parameters:\nPrior probability Likelihood Posterior probability We do this by analysing the data we have. The dataset contains the words in the mail and their counts, and the number of spam mails we have.\nLet\u0026rsquo;s say we recieve 100 mails, 20 spam mails, and 80 normal mails.\nNow, the prior probability is the probability of a mail being spam. It is calculated by dividing the number of spam mails by the total number of mails. So,\n$$P(spam) = \\frac{20}{100} = 0.2$$\nLet\u0026rsquo;s assume that the normal mails we received have words like \u0026ldquo;dear\u0026rdquo;, \u0026ldquo;friend\u0026rdquo;, and \u0026ldquo;hi\u0026rdquo;. And the spam mails have words like \u0026ldquo;money\u0026rdquo;, \u0026ldquo;free\u0026rdquo;, and \u0026ldquo;sign\u0026rdquo;.\nThe likelihood is the probability of a word being in the mail. It is calculated by multiplying the frequency of the word by the prior probability. For sake of simplicity, let\u0026rsquo;s assume that the liklihood of a word \u0026ldquo;friend\u0026rdquo; is low and the liklihood of a word \u0026ldquo;money\u0026rdquo; is high.\n$$P(\u0026ldquo;friend\u0026rdquo; | spam) = 0.02$$ $$P(\u0026ldquo;money\u0026rdquo; | spam) = 0.8$$ $$P(\u0026ldquo;hi\u0026rdquo; | spam) = \u0026hellip;$$\nAnd so on for all the words.\nThe posterior probability is the probability of a mail being spam given a word. It is calculated by multiplying the likelihood by the prior probability.\nWhen we receive a new mail, we count the frequency of each word in the mail and use that to calculate the posterior probability.\nFor example, let\u0026rsquo;s calculate the posterior probability for an email containing the words \u0026ldquo;spam\u0026rdquo;, \u0026ldquo;money\u0026rdquo;, and \u0026ldquo;money\u0026rdquo;.\nWe already know the following:\nPrior probability \\( P(\\text{spam}) = 0.2 \\). Likelihood \\( P(\\text{spam} \\mid \\text{\u0026ldquo;spam\u0026rdquo;}) = 0.7 \\) and \\( P(\\text{spam} \\mid \\text{\u0026ldquo;money\u0026rdquo;}) = 0.8 \\). Now, using Naive Bayes, we assume the words are conditionally independent. So the likelihood of the words \u0026ldquo;spam\u0026rdquo;, \u0026ldquo;money\u0026rdquo;, and \u0026ldquo;money\u0026rdquo; appearing in the spam class is:\n\\[ P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;} \\mid \\text{spam}) = \\] \\[P(\\text{\u0026ldquo;spam\u0026rdquo;} \\mid \\text{spam}) \\times P(\\text{\u0026ldquo;money\u0026rdquo;} \\mid \\text{spam}) \\times P(\\text{\u0026ldquo;money\u0026rdquo;} \\mid \\text{spam}) \\] Then, \\[ P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;} \\mid \\text{spam}) = 0.7 \\times 0.8 \\times 0.8 = 0.448 \\]\nNext, we calculate the evidence \\( P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}) \\). This is the total probability of seeing the words \u0026ldquo;spam\u0026rdquo;, \u0026ldquo;money\u0026rdquo;, and \u0026ldquo;money\u0026rdquo; in any email, whether it’s spam or not. For simplicity, let\u0026rsquo;s assume the likelihoods for non-spam emails are:\n\\( P(\\text{\u0026ldquo;spam\u0026rdquo;} \\mid \\text{not spam}) = 0.1 \\) \\( P(\\text{\u0026ldquo;money\u0026rdquo;} \\mid \\text{not spam}) = 0.2 \\) Thus, the likelihood of seeing these words in a non-spam email is: $$ P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;} | \\text{not spam}) = 0.1 \\times 0.2 \\times 0.2 = 0.004 $$\nNow, calculate the evidence: \\[ P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}) = (P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;} \\mid \\text{spam}) \\times P(\\text{spam})) + (P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;} \\mid \\text{not spam}) \\times P(\\text{not spam})) \\] \\[ P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}) = (0.448 \\times 0.2) + (0.004 \\times 0.8)\\] \\[ = 0.0896 + 0.0032 = 0.0928 \\]\nFinally, we compute the posterior probability: \\[ P(\\text{spam} \\mid \\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;})\\] \\[ = \\frac{P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;} \\mid \\text{spam}) \\times P(\\text{spam})}{P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;})} \\] Then, \\[ P(\\text{spam} \\mid \\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;})\\] \\[ = \\frac{0.448 \\times 0.2}{0.0928} = \\frac{0.0896}{0.0928} \\approx 0.965 \\]\nThus, the posterior probability that the email is spam, given the words \u0026ldquo;spam\u0026rdquo;, \u0026ldquo;money\u0026rdquo;, and \u0026ldquo;money\u0026rdquo;, is approximately 0.965, or 96.5%.\nConclusion #\rIn this post, we\u0026rsquo;ve implemented the Naive Bayes classifier for filtering spam emails and demonstrated how to calculate the posterior probability using Bayes\u0026rsquo; Theorem. With this example, you can see how the Naive Bayes classifier applies the prior probability, likelihood of words, and posterior probability to categorize new emails effectively.\nWhile Naive Bayes assumes independence between features (in this case, words), which may not always hold in real-life datasets, it still performs well in practice and is computationally efficient. It\u0026rsquo;s a powerful yet simple tool for many machine learning tasks, such as spam filtering, sentiment analysis, and text classification.\nBy continually updating our beliefs based on new data (words in emails), the Naive Bayes classifier helps us make informed decisions and efficiently identify spam emails with high accuracy.\n","date":"12 October 2024","externalUrl":null,"permalink":"/blog/naive-bayes/","section":"Blog","summary":"Overview #\rBayes\u0026rsquo; Theorem is a fundamental concept in Probability Theory.","title":"Bayes' Theorem in Machine Learning","type":"blog"},{"content":" ","date":"12 October 2024","externalUrl":null,"permalink":"/blog/","section":"Blog","summary":" ","title":"Blog","type":"blog"},{"content":"","date":"12 October 2024","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"","date":"12 October 2024","externalUrl":null,"permalink":"/","section":"nveshaan","summary":"","title":"nveshaan","type":"page"},{"content":"","date":"12 October 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"\rOverview #\rIn the previous post, I trained a neural network to classify handwritten digits, achieving an accuracy of approximately 85% on the test data. The training process took more than an hour. To expedite the training process, I will now vectorize the implementation.\nImplementation #\rInstead of iterating through the training examples one by one, I will stack them up in a matrix X and one-hot encode the labels in Y. Then I can multiply X with W and add B to get the output y.\nBefore vectorising, I multiplied the weights with the training examples and added the bias values in the following way\n$$ { \\begin{bmatrix} w_{11} \u0026amp; w_{12} \u0026amp; w_{13} \u0026amp; w_{14}\\newline w_{21} \u0026amp; w_{22} \u0026amp; w_{23} \u0026amp; w_{24}\\newline w_{31} \u0026amp; w_{32} \u0026amp; w_{33} \u0026amp; w_{34} \\end{bmatrix} \\begin{bmatrix} x_{11}\\newline x_{12}\\newline x_{13}\\newline x_{14} \\end{bmatrix} + \\begin{bmatrix} b_1\\newline b_2\\newline b_3 \\end{bmatrix} } $$\n$$ { = \\begin{bmatrix} w_{11}x_{11}+w_{12}x_{12}+w_{13}x_{13}+w_{14}x_{14}+b_1\\newline w_{21}x_{11}+w_{22}x_{12}+w_{23}x_{13}+w_{24}x_{14}+b_2\\newline w_{31}x_{11}+w_{32}x_{12}+w_{33}x_{13}+w_{34}x_{14}+b_3 \\end{bmatrix} = \\begin{bmatrix} a_1\\newline a_2\\newline a_3 \\end{bmatrix} } $$\nNow, I can multiply all the training examples with the weights and add the bias values in one step\n$$ { \\begin{bmatrix} w_{11} \u0026amp; w_{12} \u0026amp; w_{13} \u0026amp; w_{14}\\newline w_{21} \u0026amp; w_{22} \u0026amp; w_{23} \u0026amp; w_{24}\\newline w_{31} \u0026amp; w_{32} \u0026amp; w_{33} \u0026amp; w_{34} \\end{bmatrix} \\begin{bmatrix} x_{11} \u0026amp; x_{21} \u0026amp; x_{31} \u0026amp; x_{41}\\newline x_{12} \u0026amp; x_{22} \u0026amp; x_{32} \u0026amp; x_{42}\\newline x_{13} \u0026amp; x_{23} \u0026amp; x_{33} \u0026amp; x_{43}\\newline x_{14} \u0026amp; x_{24} \u0026amp; x_{34} \u0026amp; x_{44} \\end{bmatrix} + \\begin{bmatrix} b_1\\newline b_2\\newline b_3 \\end{bmatrix} } $$\n$$ { = \\begin{bmatrix} a_{11} \u0026amp; a_{21} \u0026amp; a_{31} \u0026amp; a_{41}\\newline a_{12} \u0026amp; a_{22} \u0026amp; a_{32} \u0026amp; a_{42}\\newline a_{13} \u0026amp; a_{23} \u0026amp; a_{33} \u0026amp; a_{43} \\end{bmatrix} } $$\nIn the code, I got rid of one for loop.\nfor run in range(epoch): # forward prop Z1 = np.dot(W1, X) + B1 A1 = 1/(1+np.exp(-Z1+1e-5)) Z2 = np.dot(W2, A1) + B2 A2 = 1/(1+np.exp(-Z2+1e-5)) Z3 = np.dot(W3, A2) + B3 y = np.exp(Z3+1e-5) y /= sum(y) # back prop dz3 = y - Y_one dw3 = np.dot(dz3, A2.T)/m db3 = sum(dz3.T)/m da2 = np.matmul(W3.T, dz3) dz2 = A2*(1-A2)*da2 dw2 = np.dot(dz2, A1.T)/m db2 = sum(dz2.T)/m da1 = np.matmul(W2.T, dz2) dz1 = A1*(1-A1)*da1 dw1 = np.dot(dz1, X.T)/m db1 = sum(dz1.T)/m # update params W3 -= alpha*dw3 B3 -= alpha*db3.reshape(-1, 1) W2 -= alpha*dw2 B2 -= alpha*db2.reshape(-1, 1) W1 -= alpha*dw1 B1 -= alpha*db1.reshape(-1, 1) Conclusion #\rThe training process became 15x faster in the vectorised form, producing similar results to the non-vectorised form.\n","date":"30 July 2024","externalUrl":null,"permalink":"/blog/vectorised-digit-classifier/","section":"Blog","summary":"Overview #\rIn the previous post, I trained a neural network to classify handwritten digits, achieving an accuracy of approximately 85% on the test data.","title":"Vectorising the Digit Classifier","type":"blog"},{"content":"\rOverview #\rIn this post, I’ll demonstrate how to build a neural network from scratch without relying on popular ML frameworks like PyTorch, TensorFlow, or Keras. Instead, I’ll use Python libraries such as numpy, pandas, and matplotlib to develop a model that classifies handwritten digits.\nWhy implement a Neural Net from scratch? #\rPlenty of ML frameworks offer out-of-the-box functionality for building neural networks. However, implementing one from scratch is a valuable exercise. It helps you understand how neural networks work under the hood, which is essential for designing effective models.\nFor a deep dive into neural networks, check out 3Blue1Brown\u0026rsquo;s series. Here, I\u0026rsquo;ll focus on the practical implementation.\rArchitecture #\rThe model will consist of an input layer of 784 neurons, two hidden layers with 16 neurons each and an output layer with 10 neurons. This is a very simple configuration relative to modern standards.\nBoth hidden layers use sigmoid as the activation function. The final layer goes through a softmax function. The cost function is categorical cross entropy.\nThe model uses batch gradient descent algorithm to find the minima of cost function.\nImplementation #\rSetup #\rAs mentioned above, I will import the required Python libraries.\nimport numpy as np import pandas as pd import matplotlib as plt Then, I will split the data into train and test sets, taking m = 30000 as number of training examples.\ndata = pd.read_csv(\u0026#39;/data.csv\u0026#39;) train = data[:30000] test = data[30000:] X = train.drop(columns=[\u0026#39;label\u0026#39;]).transpose() # input Y = train[\u0026#39;label\u0026#39;] # output X_t = test.drop(columns=[\u0026#39;label\u0026#39;]).transpose() # input Y_t = test[\u0026#39;label\u0026#39;] # output Now, I will one-hot encode the labels\nY_one = np.zeros((m, 10)) for i in range(m): Y_one[i][Y[i]] = 1 Y_one = Y_one.T and initialise weights and biases. np.random.rand() generates random values in [0, 1], I\u0026rsquo;ll subtract them with 0.5 for better performance\nW1 = np.random.rand(16, 784) - 0.5 # 16x784 matrix B1 = np.random.rand(16) - 0.5 # 16x1 matrix W2 = np.random.rand(16, 16) - 0.5 # 16x16 matrix B2 = np.random.rand(16) - 0.5 # 16x1 matrix W3 = np.random.rand(10, 16) - 0.5 # 10X16 matrix B3 = np.random.rand(10) - 0.5 # 10x1 matrix At first, these parameters are just random numbers. When used, they produce garbage results. As the model learns to predict the correct values, it tunes them to reasonable numbers. Which, when used, will yield good results.\nTraining #\rBefore going into the training process, I will discuss each part separately.\nForward Prop #\rI will feed the training examples to the input layers, multiplying them by the weights and adding the bias values. This output is then input into the first hidden layer, and this process continues to the final output layer.\nZ1 = np.dot(W1, X[i]) + B1 A1 = 1/(1+np.exp(-Z1+1e-5)) # sigmoid Z2 = np.dot(W2, A1) + B2 A2 = 1/(1+np.exp(-Z2+1e-5)) # sigmoid Z3 = np.dot(W3, A2) + B3 y = np.exp(Z3+1e-5) y /= sum(y) # softmax # I added a small value of 10^-5 to prevent the exponent function from vanishing Back Prop #\rBy applying the chain rule of calculus, I will compute the partial derivatives for each node in the layers. However, numpy makes implementing backpropagation much simpler.\ndz3 = y - Y_one.T[i] dw3 += np.outer(dz3, A2) db3 += dz3 da2 = np.dot(W3.T, dz3) dz2 = A2*(1 - A2)*da2 dw2 += np.outer(dz2, A1) db2 += dz2 da1 = np.dot(W2.T, dz2) dz1 = A1*(1 - A1)*da1 dw1 += np.outer(dz1, X[i]) db1 += dz1 I\u0026rsquo;m summing all the increments and decrements to the weights and biases across all training examples and updating them at the end of the epoch.\nUpdate Params #\rI\u0026rsquo;ll adjust the parameters once I\u0026rsquo;ve gone through all the training examples.\nW3 -= alpha*dw3/m B3 -= alpha*db3/m W2 -= alpha*dw2/m B2 -= alpha*db2/m W1 -= alpha*dw1/m B1 -= alpha*db1/m I will repeat the process through multiple iterations (or epochs) to find the minimum value.\nPutting it all together #\rfor run in range(epoch): dw1 = np.zeros((16, 784)) db1 = np.zeros(16) dw2 = np.zeros((16, 16)) db2 = np.zeros(16) dw3 = np.zeros((10, 16)) db3 = np.zeros(10) for i in range(m): # Forward prop Z1 = np.dot(W1, X[i]) + B1 A1 = 1/(1+np.exp(-Z1+1e-5)) Z2 = np.dot(W2, A1) + B2 A2 = 1/(1+np.exp(-Z2+1e-5)) Z3 = np.dot(W3, A2) + B3 y = np.exp(Z3+1e-5) y /= sum(y) # Back prop dz3 = y - Y_one.T[i] dw3 += np.outer(dz3, A2) db3 += dz3 da2 = np.dot(W3.T, dz3) dz2 = A2*(1 - A2)*da2 dw2 += np.outer(dz2, A1) db2 += dz2 da1 = np.dot(W2.T, dz2) dz1 = A1*(1 - A1)*da1 dw1 += np.outer(dz1, X[i]) db1 += dz1 # Update params W3 -= alpha*dw3/m B3 -= alpha*db3/m W2 -= alpha*dw2/m B2 -= alpha*db2/m W1 -= alpha*dw1/m B1 -= alpha*db1/m Results #\rI obtained the following results after training the model for epoch = 500 with a learning rate alpha = 0.8.\nTraining accuracy: 87.22 %\nTest accuracy: 85.65 %\nThe model generalises the test data very well. The cost v/s epoch plot shows that the cost function converged to a value.\nI executed the code in Google Colab. The link to that notebook is here.\n","date":"21 July 2024","externalUrl":null,"permalink":"/blog/digit-classifier/","section":"Blog","summary":"Overview #\rIn this post, I’ll demonstrate how to build a neural network from scratch without relying on popular ML frameworks like PyTorch, TensorFlow, or Keras.","title":"Coding a Neural Network from scratch","type":"blog"},{"content":"Hey👋, I’m an undergraduate at IISER Thiruvananthapuram, studying Mathematics, Physics, and Data Science. I created this website to share my knowledge and experiences, hoping it will serve as a platform to showcase my academic projects, coding endeavours, and insights gained during my studies.\nHow did it all start? My school introduced me to programming at the age of 13. I started coding in Java and wrote several programs as part of my assignments. One of my early projects was a program that translated Morse code into English, which impressed my teacher. My friends and I then collaborated on another program that translated our secret code language.\nWhen the pandemic hit, I was locked down at home and took the opportunity to expand my skills further. I learned how to create websites, explored Python, and discovered Manim, a library for animating mathematical concepts.\nOver the years, I have explored various fields, but my primary interest lies in, Currently, I am delving into artificial intelligence, working on small projects to apply and expand my knowledge in this exciting area.\nAside from my academic pursuits, I enjoy playing badminton. I love listening to Shreya Ghoshal\u0026rsquo;s songs (also Cherry\u0026rsquo;s songs). I\u0026rsquo;m passionate about cooking, and I enjoy travelling and going out on adventures.\nI have also taken interest in cars, motorcycles and rockets !!!\n","externalUrl":null,"permalink":"/about/","section":"nveshaan","summary":"Hey👋, I’m an undergraduate at IISER Thiruvananthapuram, studying Mathematics, Physics, and Data Science.","title":"About","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"\rOverview #\rThe project is a simple navigater that can navigate through a maze without human intervention. It has been designed to be a simple, easy-to-use navigater for navigating through a maze.\n","externalUrl":null,"permalink":"/projects/autonomous-navigater/","section":"Projects","summary":"Overview #\rThe project is a simple navigater that can navigate through a maze without human intervention.","title":"Autonomous Navigater","type":"projects"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"\rOverview #\rMinimax is a popular algorithm used in the game of chess. It is also used in many other games such as checkers, checkers, checkers, and checkers. Minimax is a fundamental concept in the game of chess.\n","externalUrl":null,"permalink":"/projects/chess-engine/","section":"Projects","summary":"Overview #\rMinimax is a popular algorithm used in the game of chess.","title":"Chess Engine","type":"projects"},{"content":"Feel free to connect with me:\nGitHub LinkedIn Email (nveshaan23@iisertvm.ac.in) CV (PDF) ","externalUrl":null,"permalink":"/contact/","section":"nveshaan","summary":"Feel free to connect with me:","title":"Contact","type":"page"},{"content":" Overview #\rElectromyography (EMG) measures the electrical activity generated by skeletal muscles during contractions, typically using surface electrodes. This technique enables the identification of specific gestures by analyzing the signals, making it particularly useful for applications in human-computer interaction. By interpreting muscle activity, devices can effectively respond to user intentions, thereby enhancing assistive technologies for individuals with mobility impairments.\nTo achieve high-accuracy human-computer interaction while minimizing the calibration data required from new users, a neural network is trained to classify gestures from EMG signals. This approach ensures seamless integration, allowing devices to adapt quickly to individual users and provide a more intuitive and responsive experience.\nThe project is divided into three main parts:\nData Collection Model Design and Training Real Time Interface Data Acquisition #\rThe dataset is recorded from 13 subjects. Each subject has 1000 samples, totalling to 13,000. The labels are the states of their dominant fist (open or closed). Both labels have equal number of examples. Below is the distribution of the dataset.\n","externalUrl":null,"permalink":"/projects/muscle-wave-classifier/","section":"Projects","summary":"Overview #\rElectromyography (EMG) measures the electrical activity generated by skeletal muscles during contractions, typically using surface electrodes.","title":"Muscle Wave Classifier","type":"projects"},{"content":" ","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":" ","title":"Projects","type":"projects"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]