
[{"content":"\rOverview #\rBayes\u0026rsquo; Theorem is a fundamental concept in Probability Theory. It is widely used in fields such as statistics, machine learning, and data science, especially in the context of probabilistic inference and decision-making. In this post, I will explain how Bayes\u0026rsquo; Theorem is used in Machine Learning, by considering a simple example. But, before that, let\u0026rsquo;s understand some terminology.\nTerminology #\rBayes\u0026rsquo; Theorem #\rIt describes how to update the probability of a hypothesis based on new evidence. It provides a way to calculate the posterior probability of an event by combining the prior probability with the likelihood.\nMathematically, Bayes\u0026rsquo; Theorem is defined as:\n$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\nLikelihood vs. Probability #\rProbability measures how likely a particular event is to occur, given a certain model or assumption. In Bayes\u0026rsquo; Theorem, \\( P(B) \\) represents the total probability of observing event \\(B\\).\nLikelihood measures how likely the observed data is given a particular hypothesis or model. It\u0026rsquo;s similar to probability with a subtle difference. Probability is about predicting outcomes, while likelihood is about fitting a model to data. In Bayes\u0026rsquo; Theorem, \\( P(B|A)\\) is the likelihood of observing data \\(B\\) given the hypothesis \\(A\\).\nPrior Probability #\rThe prior probability represents your initial belief about the probability of a hypothesis before you have any new data or evidence. In Bayes\u0026rsquo; Theorem, \\( P(A) \\) is the prior probability of the hypothesis \\(A\\).\nPosterior Probability #\rThe posterior probability is the updates probability of a hypothesis after observing new evidence. It combines the prior belief with the likelihood of the observed data. In Bayes\u0026rsquo; Theorem, \\( P(A|B) \\) is the posterior probability of the hypothesis \\(A\\) given the evidence \\(B\\).\nImplementation #\rOne of the use-cases of Naive Bayes\u0026rsquo; Classifier is filtering out spam mails from your inbox. Let\u0026rsquo;s see how it is implemented.\nFor a classifier, we need to define three parameters:\nPrior probability Likelihood Posterior probability We do this by analysing the data we have. The dataset contains the words in the mail and their counts, and the number of spam mails we have.\nLet\u0026rsquo;s say we recieve 100 mails, 20 spam mails, and 80 normal mails.\nNow, the prior probability is the probability of a mail being spam. It is calculated by dividing the number of spam mails by the total number of mails. So,\n$$P(spam) = \\frac{20}{100} = 0.2$$\nLet\u0026rsquo;s assume that the normal mails we received have words like \u0026ldquo;dear\u0026rdquo;, \u0026ldquo;friend\u0026rdquo;, and \u0026ldquo;hi\u0026rdquo;. And the spam mails have words like \u0026ldquo;money\u0026rdquo;, \u0026ldquo;free\u0026rdquo;, and \u0026ldquo;sign\u0026rdquo;.\nThe likelihood is the probability of a word being in the mail. It is calculated by multiplying the frequency of the word by the prior probability. For sake of simplicity, let\u0026rsquo;s assume that the liklihood of a word \u0026ldquo;friend\u0026rdquo; is low and the liklihood of a word \u0026ldquo;money\u0026rdquo; is high.\n$$P(\u0026ldquo;friend\u0026rdquo; | spam) = 0.02$$ $$P(\u0026ldquo;money\u0026rdquo; | spam) = 0.8$$ $$P(\u0026ldquo;hi\u0026rdquo; | spam) = \u0026hellip;$$\nAnd so on for all the words.\nThe posterior probability is the probability of a mail being spam given a word. It is calculated by multiplying the likelihood by the prior probability.\nWhen we receive a new mail, we count the frequency of each word in the mail and use that to calculate the posterior probability.\nFor example, let\u0026rsquo;s calculate the posterior probability for an email containing the words \u0026ldquo;spam\u0026rdquo;, \u0026ldquo;money\u0026rdquo;, and \u0026ldquo;money\u0026rdquo;.\nWe already know the following:\nPrior probability \\( P(\\text{spam}) = 0.2 \\). Likelihood \\( P(\\text{spam} \\mid \\text{\u0026ldquo;spam\u0026rdquo;}) = 0.7 \\) and \\( P(\\text{spam} \\mid \\text{\u0026ldquo;money\u0026rdquo;}) = 0.8 \\). Now, using Naive Bayes, we assume the words are conditionally independent. So the likelihood of the words \u0026ldquo;spam\u0026rdquo;, \u0026ldquo;money\u0026rdquo;, and \u0026ldquo;money\u0026rdquo; appearing in the spam class is:\n\\[ P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;} \\mid \\text{spam}) = \\] \\[P(\\text{\u0026ldquo;spam\u0026rdquo;} \\mid \\text{spam}) \\times P(\\text{\u0026ldquo;money\u0026rdquo;} \\mid \\text{spam}) \\times P(\\text{\u0026ldquo;money\u0026rdquo;} \\mid \\text{spam}) \\] Then, \\[ P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;} \\mid \\text{spam}) = 0.7 \\times 0.8 \\times 0.8 = 0.448 \\]\nNext, we calculate the evidence \\( P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}) \\). This is the total probability of seeing the words \u0026ldquo;spam\u0026rdquo;, \u0026ldquo;money\u0026rdquo;, and \u0026ldquo;money\u0026rdquo; in any email, whether it’s spam or not. For simplicity, let\u0026rsquo;s assume the likelihoods for non-spam emails are:\n\\( P(\\text{\u0026ldquo;spam\u0026rdquo;} \\mid \\text{not spam}) = 0.1 \\) \\( P(\\text{\u0026ldquo;money\u0026rdquo;} \\mid \\text{not spam}) = 0.2 \\) Thus, the likelihood of seeing these words in a non-spam email is: $$ P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;} | \\text{not spam}) = 0.1 \\times 0.2 \\times 0.2 = 0.004 $$\nNow, calculate the evidence: \\[ P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}) = (P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;} \\mid \\text{spam}) \\times P(\\text{spam})) + (P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;} \\mid \\text{not spam}) \\times P(\\text{not spam})) \\] \\[ P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}) = (0.448 \\times 0.2) + (0.004 \\times 0.8)\\] \\[ = 0.0896 + 0.0032 = 0.0928 \\]\nFinally, we compute the posterior probability: \\[ P(\\text{spam} \\mid \\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;})\\] \\[ = \\frac{P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;} \\mid \\text{spam}) \\times P(\\text{spam})}{P(\\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;})} \\] Then, \\[ P(\\text{spam} \\mid \\text{\u0026ldquo;spam\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;}, \\text{\u0026ldquo;money\u0026rdquo;})\\] \\[ = \\frac{0.448 \\times 0.2}{0.0928} = \\frac{0.0896}{0.0928} \\approx 0.965 \\]\nThus, the posterior probability that the email is spam, given the words \u0026ldquo;spam\u0026rdquo;, \u0026ldquo;money\u0026rdquo;, and \u0026ldquo;money\u0026rdquo;, is approximately 0.965, or 96.5%.\nConclusion #\rIn this post, we\u0026rsquo;ve implemented the Naive Bayes classifier for filtering spam emails and demonstrated how to calculate the posterior probability using Bayes\u0026rsquo; Theorem. With this example, you can see how the Naive Bayes classifier applies the prior probability, likelihood of words, and posterior probability to categorize new emails effectively.\nWhile Naive Bayes assumes independence between features (in this case, words), which may not always hold in real-life datasets, it still performs well in practice and is computationally efficient. It\u0026rsquo;s a powerful yet simple tool for many machine learning tasks, such as spam filtering, sentiment analysis, and text classification.\nBy continually updating our beliefs based on new data (words in emails), the Naive Bayes classifier helps us make informed decisions and efficiently identify spam emails with high accuracy.\n","date":"12 October 2024","externalUrl":null,"permalink":"/blog/naive-bayes/","section":"Blog","summary":"Overview #\rBayes\u0026rsquo; Theorem is a fundamental concept in Probability Theory.","title":"Bayes' Theorem in Machine Learning","type":"blog"},{"content":" ","date":"12 October 2024","externalUrl":null,"permalink":"/blog/","section":"Blog","summary":" ","title":"Blog","type":"blog"},{"content":"","date":"12 October 2024","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"","date":"12 October 2024","externalUrl":null,"permalink":"/","section":"nveshaan","summary":"","title":"nveshaan","type":"page"},{"content":"","date":"12 October 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"\rOverview #\rIn the previous post, I trained a neural network to classify handwritten digits, achieving an accuracy of approximately 85% on the test data. The training process took more than an hour. To expedite the training process, I will now vectorize the implementation.\nImplementation #\rInstead of iterating through the training examples one by one, I will stack them up in a matrix X and one-hot encode the labels in Y. Then I can multiply X with W and add B to get the output y.\nBefore vectorising, I multiplied the weights with the training examples and added the bias values in the following way\n$$ { \\begin{bmatrix} w_{11} \u0026amp; w_{12} \u0026amp; w_{13} \u0026amp; w_{14}\\newline w_{21} \u0026amp; w_{22} \u0026amp; w_{23} \u0026amp; w_{24}\\newline w_{31} \u0026amp; w_{32} \u0026amp; w_{33} \u0026amp; w_{34} \\end{bmatrix} \\begin{bmatrix} x_{11}\\newline x_{12}\\newline x_{13}\\newline x_{14} \\end{bmatrix} + \\begin{bmatrix} b_1\\newline b_2\\newline b_3 \\end{bmatrix} } $$\n$$ { = \\begin{bmatrix} w_{11}x_{11}+w_{12}x_{12}+w_{13}x_{13}+w_{14}x_{14}+b_1\\newline w_{21}x_{11}+w_{22}x_{12}+w_{23}x_{13}+w_{24}x_{14}+b_2\\newline w_{31}x_{11}+w_{32}x_{12}+w_{33}x_{13}+w_{34}x_{14}+b_3 \\end{bmatrix} = \\begin{bmatrix} a_1\\newline a_2\\newline a_3 \\end{bmatrix} } $$\nNow, I can multiply all the training examples with the weights and add the bias values in one step\n$$ { \\begin{bmatrix} w_{11} \u0026amp; w_{12} \u0026amp; w_{13} \u0026amp; w_{14}\\newline w_{21} \u0026amp; w_{22} \u0026amp; w_{23} \u0026amp; w_{24}\\newline w_{31} \u0026amp; w_{32} \u0026amp; w_{33} \u0026amp; w_{34} \\end{bmatrix} \\begin{bmatrix} x_{11} \u0026amp; x_{21} \u0026amp; x_{31} \u0026amp; x_{41}\\newline x_{12} \u0026amp; x_{22} \u0026amp; x_{32} \u0026amp; x_{42}\\newline x_{13} \u0026amp; x_{23} \u0026amp; x_{33} \u0026amp; x_{43}\\newline x_{14} \u0026amp; x_{24} \u0026amp; x_{34} \u0026amp; x_{44} \\end{bmatrix} + \\begin{bmatrix} b_1\\newline b_2\\newline b_3 \\end{bmatrix} } $$\n$$ { = \\begin{bmatrix} a_{11} \u0026amp; a_{21} \u0026amp; a_{31} \u0026amp; a_{41}\\newline a_{12} \u0026amp; a_{22} \u0026amp; a_{32} \u0026amp; a_{42}\\newline a_{13} \u0026amp; a_{23} \u0026amp; a_{33} \u0026amp; a_{43} \\end{bmatrix} } $$\nIn the code, I got rid of one for loop.\nfor run in range(epoch): # forward prop Z1 = np.dot(W1, X) + B1 A1 = 1/(1+np.exp(-Z1+1e-5)) Z2 = np.dot(W2, A1) + B2 A2 = 1/(1+np.exp(-Z2+1e-5)) Z3 = np.dot(W3, A2) + B3 y = np.exp(Z3+1e-5) y /= sum(y) # back prop dz3 = y - Y_one dw3 = np.dot(dz3, A2.T)/m db3 = sum(dz3.T)/m da2 = np.matmul(W3.T, dz3) dz2 = A2*(1-A2)*da2 dw2 = np.dot(dz2, A1.T)/m db2 = sum(dz2.T)/m da1 = np.matmul(W2.T, dz2) dz1 = A1*(1-A1)*da1 dw1 = np.dot(dz1, X.T)/m db1 = sum(dz1.T)/m # update params W3 -= alpha*dw3 B3 -= alpha*db3.reshape(-1, 1) W2 -= alpha*dw2 B2 -= alpha*db2.reshape(-1, 1) W1 -= alpha*dw1 B1 -= alpha*db1.reshape(-1, 1) Conclusion #\rThe training process became 15x faster in the vectorised form, producing similar results to the non-vectorised form.\n","date":"30 July 2024","externalUrl":null,"permalink":"/blog/vectorised-digit-classifier/","section":"Blog","summary":"Overview #\rIn the previous post, I trained a neural network to classify handwritten digits, achieving an accuracy of approximately 85% on the test data.","title":"Vectorising the Digit Classifier","type":"blog"},{"content":"\rOverview #\rIn this post, I’ll demonstrate how to build a neural network from scratch without relying on popular ML frameworks like PyTorch, TensorFlow, or Keras. Instead, I’ll use Python libraries such as numpy, pandas, and matplotlib to develop a model that classifies handwritten digits.\nWhy implement a Neural Net from scratch? #\rPlenty of ML frameworks offer out-of-the-box functionality for building neural networks. However, implementing one from scratch is a valuable exercise. It helps you understand how neural networks work under the hood, which is essential for designing effective models.\nFor a deep dive into neural networks, check out 3Blue1Brown\u0026rsquo;s series. Here, I\u0026rsquo;ll focus on the practical implementation.\rArchitecture #\rThe model will consist of an input layer of 784 neurons, two hidden layers with 16 neurons each and an output layer with 10 neurons. This is a very simple configuration relative to modern standards.\nBoth hidden layers use sigmoid as the activation function. The final layer goes through a softmax function. The cost function is categorical cross entropy.\nThe model uses batch gradient descent algorithm to find the minima of cost function.\nImplementation #\rSetup #\rAs mentioned above, I will import the required Python libraries.\nimport numpy as np import pandas as pd import matplotlib as plt Then, I will split the data into train and test sets, taking m = 30000 as number of training examples.\ndata = pd.read_csv(\u0026#39;/data.csv\u0026#39;) train = data[:30000] test = data[30000:] X = train.drop(columns=[\u0026#39;label\u0026#39;]).transpose() # input Y = train[\u0026#39;label\u0026#39;] # output X_t = test.drop(columns=[\u0026#39;label\u0026#39;]).transpose() # input Y_t = test[\u0026#39;label\u0026#39;] # output Now, I will one-hot encode the labels\nY_one = np.zeros((m, 10)) for i in range(m): Y_one[i][Y[i]] = 1 Y_one = Y_one.T and initialise weights and biases. np.random.rand() generates random values in [0, 1], I\u0026rsquo;ll subtract them with 0.5 for better performance\nW1 = np.random.rand(16, 784) - 0.5 # 16x784 matrix B1 = np.random.rand(16) - 0.5 # 16x1 matrix W2 = np.random.rand(16, 16) - 0.5 # 16x16 matrix B2 = np.random.rand(16) - 0.5 # 16x1 matrix W3 = np.random.rand(10, 16) - 0.5 # 10X16 matrix B3 = np.random.rand(10) - 0.5 # 10x1 matrix At first, these parameters are just random numbers. When used, they produce garbage results. As the model learns to predict the correct values, it tunes them to reasonable numbers. Which, when used, will yield good results.\nTraining #\rBefore going into the training process, I will discuss each part separately.\nForward Prop #\rI will feed the training examples to the input layers, multiplying them by the weights and adding the bias values. This output is then input into the first hidden layer, and this process continues to the final output layer.\nZ1 = np.dot(W1, X[i]) + B1 A1 = 1/(1+np.exp(-Z1+1e-5)) # sigmoid Z2 = np.dot(W2, A1) + B2 A2 = 1/(1+np.exp(-Z2+1e-5)) # sigmoid Z3 = np.dot(W3, A2) + B3 y = np.exp(Z3+1e-5) y /= sum(y) # softmax # I added a small value of 10^-5 to prevent the exponent function from vanishing Back Prop #\rBy applying the chain rule of calculus, I will compute the partial derivatives for each node in the layers. However, numpy makes implementing backpropagation much simpler.\ndz3 = y - Y_one.T[i] dw3 += np.outer(dz3, A2) db3 += dz3 da2 = np.dot(W3.T, dz3) dz2 = A2*(1 - A2)*da2 dw2 += np.outer(dz2, A1) db2 += dz2 da1 = np.dot(W2.T, dz2) dz1 = A1*(1 - A1)*da1 dw1 += np.outer(dz1, X[i]) db1 += dz1 I\u0026rsquo;m summing all the increments and decrements to the weights and biases across all training examples and updating them at the end of the epoch.\nUpdate Params #\rI\u0026rsquo;ll adjust the parameters once I\u0026rsquo;ve gone through all the training examples.\nW3 -= alpha*dw3/m B3 -= alpha*db3/m W2 -= alpha*dw2/m B2 -= alpha*db2/m W1 -= alpha*dw1/m B1 -= alpha*db1/m I will repeat the process through multiple iterations (or epochs) to find the minimum value.\nPutting it all together #\rfor run in range(epoch): dw1 = np.zeros((16, 784)) db1 = np.zeros(16) dw2 = np.zeros((16, 16)) db2 = np.zeros(16) dw3 = np.zeros((10, 16)) db3 = np.zeros(10) for i in range(m): # Forward prop Z1 = np.dot(W1, X[i]) + B1 A1 = 1/(1+np.exp(-Z1+1e-5)) Z2 = np.dot(W2, A1) + B2 A2 = 1/(1+np.exp(-Z2+1e-5)) Z3 = np.dot(W3, A2) + B3 y = np.exp(Z3+1e-5) y /= sum(y) # Back prop dz3 = y - Y_one.T[i] dw3 += np.outer(dz3, A2) db3 += dz3 da2 = np.dot(W3.T, dz3) dz2 = A2*(1 - A2)*da2 dw2 += np.outer(dz2, A1) db2 += dz2 da1 = np.dot(W2.T, dz2) dz1 = A1*(1 - A1)*da1 dw1 += np.outer(dz1, X[i]) db1 += dz1 # Update params W3 -= alpha*dw3/m B3 -= alpha*db3/m W2 -= alpha*dw2/m B2 -= alpha*db2/m W1 -= alpha*dw1/m B1 -= alpha*db1/m Results #\rI obtained the following results after training the model for epoch = 500 with a learning rate alpha = 0.8.\nTraining accuracy: 87.22 %\nTest accuracy: 85.65 %\nThe model generalises the test data very well. The cost v/s epoch plot shows that the cost function converged to a value.\nI executed the code in Google Colab. The link to that notebook is here.\n","date":"21 July 2024","externalUrl":null,"permalink":"/blog/digit-classifier/","section":"Blog","summary":"Overview #\rIn this post, I’ll demonstrate how to build a neural network from scratch without relying on popular ML frameworks like PyTorch, TensorFlow, or Keras.","title":"Coding a Neural Network from scratch","type":"blog"},{"content":"Hey👋, I’m an undergraduate at IISER Thiruvananthapuram, majoring in Data-Science. I created this website to share my knowledge and experiences, hoping it will serve as a platform to showcase my academic projects, coding endeavours, and insights gained during my studies.\nHow did it all start? My school introduced me to programming at the age of 13. I started coding in Java, and wrote several programs as part of my assignments. One of my early projects was a program that translated Morse code into English, which impressed my teacher. My friends and I then collaborated on another program that translated our secret code language.\nWhen the pandemic hit, I was locked down at home and took the opportunity to expand my skills further. I learned how to create websites, explored Python, and discovered Manim, a library for animating mathematical concepts.\nOver the years, I have explored various fields, but my primary interest lies in, Currently, I am delving into artificial intelligence, working on small projects to apply and expand my knowledge in this exciting area.\nAside from my academic pursuits, I enjoy playing badminton. I love listening to Shreya Ghoshal\u0026rsquo;s songs (also Cherry\u0026rsquo;s songs). I\u0026rsquo;m passionate about cooking, and I enjoy travelling and going out on adventures.\nI have also taken interest in cars, bikes and rockets !!!\n","externalUrl":null,"permalink":"/about/","section":"nveshaan","summary":"Hey👋, I’m an undergraduate at IISER Thiruvananthapuram, majoring in Data-Science.","title":"About","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"\rOverview #\rThe project is a simple navigater that can navigate through a maze without human intervention. It has been designed to be a simple, easy-to-use navigater for navigating through a maze.\n","externalUrl":null,"permalink":"/projects/autonomous-navigater/","section":"Projects","summary":"Overview #\rThe project is a simple navigater that can navigate through a maze without human intervention.","title":"Autonomous Navigater","type":"projects"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"\rOverview #\rMinimax is a popular algorithm used in the game of chess. It is also used in many other games such as checkers, checkers, checkers, and checkers. Minimax is a fundamental concept in the game of chess.\n","externalUrl":null,"permalink":"/projects/chess-engine/","section":"Projects","summary":"Overview #\rMinimax is a popular algorithm used in the game of chess.","title":"Chess Engine","type":"projects"},{"content":"Feel free to connect with me:\nGitHub LinkedIn Email (nveshaan23@iisertvm.ac.in) CV (PDF) ","externalUrl":null,"permalink":"/contact/","section":"nveshaan","summary":"Feel free to connect with me:","title":"Contact","type":"page"},{"content":" Overview #\rElectromyography (EMG) measures the electrical activity generated by skeletal muscles during contractions, typically using surface electrodes. This technique enables the identification of specific gestures by analyzing the signals, making it particularly useful for applications in human-computer interaction. By interpreting muscle activity, devices can effectively respond to user intentions, thereby enhancing assistive technologies for individuals with mobility impairments.\nIn this project, a neural network is trained to classify the state of a user\u0026rsquo;s dominant fist (open or closed) using EMG data. The prediction made by the classifier is then used to control a remote-controlled car. The following sections discuss the methodology and results obtained.\nData Acquisition #\rTo record the EMG signals, the Muscle BioAmp Candy sensor was utilized, interfaced with an Arduino UNO. This setup allowed real-time recording of muscle activity, with the sensor placed on the ulnar nerve region to capture signals related to the palm gestures.\nThe dataset was collected from 13 participants, each contributing 1,000 samples, resulting in a total of 13,000 labeled EMG samples. Each sample comprises 256 data points, representing the muscle activity over time. The data was recorded with a sampling rate optimized for accurately capturing high-frequency EMG signals.\nData Collection Protocol:\nEach participant was equipped with the EMG sensor placed on their dominant arm\u0026rsquo;s forearm. Participants were provided with visual cues displayed on a screen to guide them in performing the gestures. The task involved alternating between opening and closing their fists consecutively for a fixed duration while ensuring consistency and minimal movement artifacts. The raw EMG signal was preprocessed to extract the signal envelope, a smoothed representation of the EMG activity that highlights overall muscle activation. Each sample was labeled as:\n0: Open fist 1: Closed fist The processed data is stored in .npy format, for easy loading into NumPy. The dataset is structured such that each file corresponds to a participant\u0026rsquo;s samples, with their associated labels stored in a separate array.\nThe dataset, after the preprocessing, is available for download here.\nModel Design and Training #\rThe architecture is structured in such a way that the initial input of 256 features is represented in a higher dimensional space with 512 dimensions. Then, it is passed down to much smaller dimensional spaces with 128 and 64 dimensions, to reach a single neuron. The final output is a single value between 0 and 1, representing the probability that the fist is closed. The inputs are normalized before being fed into the model.\nInput Normalization: Standardizes inputs for consistent performance. Hidden Layers: 512 neurons: Activation function - ReLU 128 neurons: Activation function - ReLU 64 neurons: Activation function - ReLU Output Layer: A single neuron with a sigmoid activation function. The model is trained using the Adam optimizer with a learning rate of 0.001. The model is trained for 25 epochs using a batch size of 32. The loss function is binary crossentropy.\nThe model is trained using the TensorFlow framework.\nimport tensorflow as tf model = tf.keras.Sequential([ tf.keras.layers.Normalization(axis=-1, input_shape=[256]), tf.keras.layers.Dense(512, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(64, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;) ]) model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) model.fit(X_train, Y_train, epochs=25, validation_split=0.2) Below are the plots of the training v/s validation accuracy,\nand the plots of the training v/s validation loss.\nThe model has converged to an acceptable minimum.\nReal-time Interface #\rA remote-controlled car is programmed using an ESP 32 to receive real-time commands from a server via Wi-Fi. Communication between the server and the ESP32 is established using Python\u0026rsquo;s socket library.\nThe server continuously collects 256 EMG data points from a user equipped with an EMG sensor. This data is then passed to the model for inference. The server transmits the control signals to the ESP32 based on the predicted gesture, enabling responsive car movements. The car moves forward when the model predicts an closed fist, and stops when it predicts a open fist. The process operates in a continuous loop until the connection is terminated.\ngraph LR\rA[/EMG Sensor + Arduino/] --\u003e|EMG Signal| B{Model}\rB --\u003e|Prediction| C(Server)\rC --\u003e|Control Signal| D(RC Car)\rD --\u003eA\rConclusion #\rThis project has been an invaluable learning experience, providing a solid foundation in integrating machine learning with real-time applications. Looking ahead, there is significant scope for further development. I plan to expand the dataset by incorporating a wider range of gestures, experiment with advanced model architectures, and explore transfer learning techniques to enhance model performance and adaptability.\nThe entire codebase for this project can be found on GitHub.\n","externalUrl":null,"permalink":"/projects/muscle-wave-classifier/","section":"Projects","summary":"Overview #\rElectromyography (EMG) measures the electrical activity generated by skeletal muscles during contractions, typically using surface electrodes.","title":"Muscle Wave Classifier","type":"projects"},{"content":" ","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":" ","title":"Projects","type":"projects"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]