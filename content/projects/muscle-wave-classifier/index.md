---
title: "Muscle Wave Classifier"
layout: "simple"
---

---

## Overview

[Electromyography](https://en.wikipedia.org/wiki/Electromyography) (EMG) measures the electrical activity generated by skeletal muscles during contractions, typically using surface electrodes. This technique enables the identification of specific gestures by analyzing the signals, making it particularly useful for applications in human-computer interaction. By interpreting muscle activity, devices can effectively respond to user intentions, thereby enhancing assistive technologies for individuals with mobility impairments.

In this project, a neural network is trained to classify the state of a user's dominant fist (open or closed) using EMG data. The prediction made by the classifier is then used to control a remote-controlled car. The following sections discuss the methodology and results obtained.

## Data Acquisition

To record the EMG signals, the [Muscle BioAmp Candy](https://docs.upsidedownlabs.tech/hardware/bioamp/muscle-bioamp-candy/) sensor was utilized, interfaced with an [Arduino UNO](https://docs.arduino.cc/hardware/uno-rev3/). This setup allowed real-time recording of muscle activity, with the sensor placed on the [ulnar nerve](https://en.wikipedia.org/wiki/Ulnar_nerve) region to capture signals related to the palm gestures.

The dataset was collected from 13 participants, each contributing 1,000 samples, resulting in a total of 13,000 labeled EMG samples. Each sample comprises 256 data points, representing the muscle activity over time. The data was recorded with a sampling rate optimized for accurately capturing high-frequency EMG signals.

**Data Collection Protocol:**

 - Each participant was equipped with the EMG sensor placed on their dominant arm's forearm.
 - Participants were provided with visual cues displayed on a screen to guide them in performing the gestures.
 - The task involved alternating between opening and closing their fists consecutively for a fixed duration while ensuring consistency and minimal movement artifacts.
 - The raw EMG signal was preprocessed to extract the signal [envelope](https://en.wikipedia.org/wiki/Envelope_(waves)#:~:text=In%20physics%20and%20engineering%2C%20the,envelope%20and%20a%20lower%20envelope.), a smoothed representation of the EMG activity that highlights overall muscle activation.

Each sample was labeled as:
 - 0: Open fist
 - 1: Closed fist

The processed data is stored in `.npy` format, for easy loading into NumPy. The dataset is structured such that each file corresponds to a participant's samples, with their associated labels stored in a separate array.

The dataset, after the preprocessing, is available for download [here](https://www.kaggle.com/datasets/nveshaan/openclose-fist-state-emg-signal-envelope).

## Model Design and Training

The architecture is structured in such a way that the initial input of 256 features is represented in a higher dimensional space with 512 dimensions. Then, it is passed down to much smaller dimensional spaces with 128 and 64 dimensions, to reach a single neuron. The final output is a single value between 0 and 1, representing the probability that the fist is closed. The inputs are normalized before being fed into the model.

 1. **Input Normalization:** Standardizes inputs for consistent performance.
 2. **Hidden Layers:**
    - **512 neurons:** Activation function - ReLU
    - **128 neurons:** Activation function - ReLU
    - **64 neurons:** Activation function - ReLU
 3. **Output Layer:** A single neuron with a sigmoid activation function.

The model is trained using the Adam optimizer with a learning rate of 0.001. The model is trained for 25 epochs using a batch size of 32. The loss function is binary crossentropy.

The model is trained using the TensorFlow framework.

```py
import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Normalization(axis=-1, input_shape=[256]),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, Y_train, epochs=25, validation_split=0.2)
```

Below are the plots of the training v/s validation accuracy,

{{< chart >}}
type: 'line',
data: {
labels: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
,
datasets: [{
label: 'Training Accuracy',
data: [0.6772500276565552, 0.734000027179718, 0.7329999804496765, 0.7397500276565552, 0.746749997138977, 0.7407500147819519, 0.7494999766349792, 0.7478749752044678, 0.7507500052452087, 0.7505000233650208, 0.7507500052452087, 0.7523750066757202, 0.753250002861023, 0.7493749856948853, 0.749875009059906, 0.7505000233650208, 0.7508749961853027, 0.7519999742507935, 0.7509999871253967, 0.7524999976158142, 0.7581250071525574, 0.7515000104904175, 0.7490000128746033, 0.7534999847412109, 0.7533749938011169]
,
borderWidth: 1,
radius: 0,
}, 
{
label: 'Validation Accuracy',
data: [0.7524999976158142, 0.7484999895095825, 0.7609999775886536, 0.7394999861717224, 0.7634999752044678, 0.7565000057220459, 0.7570000290870667, 0.7630000114440918, 0.7009999752044678, 0.7574999928474426, 0.7524999976158142, 0.7555000185966492, 0.7540000081062317, 0.7580000162124634, 0.7605000138282776, 0.7570000290870667, 0.7605000138282776, 0.7555000185966492, 0.7565000057220459, 0.7534999847412109, 0.7490000128746033, 0.7590000033378601, 0.7540000081062317, 0.7429999709129333, 0.7534999847412109]
,
borderWidth: 1,
radius: 0,
}]
},
options: {
aspectRatio: 2.5,
interaction: {
intersect: false
},
plugins: {
legend: true
},
scales: {
x: {
type: 'linear',
ticks: {
count: 6
},
display: true,
title: {
display: true,
text: 'Epoch'
}
},
y: {
type: 'linear',
ticks: {
count: 6
},
display: true,
title: {
display: true,
text: 'Accuracy'
}
}
}
},
{{< /chart >}}

and the plots of the training v/s validation loss.

{{< chart >}}
type: 'line',
data: {
labels: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
,
datasets: [{
label: 'Training Loss',
data: [0.9863097667694092, 0.5917187333106995, 0.5682947635650635, 0.5510603189468384, 0.5463647246360779, 0.5430137515068054, 0.52552330493927, 0.530414879322052, 0.5172624588012695, 0.5216422080993652, 0.5150081515312195, 0.5117608308792114, 0.5084753632545471, 0.5070698857307434, 0.5062569379806519, 0.5179017186164856, 0.5109373331069946, 0.4996016323566437, 0.49885228276252747, 0.49077117443084717, 0.4891473054885864, 0.4950791597366333, 0.48748037219047546, 0.49238449335098267, 0.48640212416648865]
,
borderWidth: 1,
radius: 0,
}, 
{
label: 'Validation Loss',
data: [0.6182988882064819, 0.569341242313385, 0.5217929482460022, 0.5452141165733337, 0.5160990357398987, 0.5144972205162048, 0.5137775540351868, 0.5113060474395752, 0.6228801608085632, 0.5002530813217163, 0.5057201981544495, 0.49339759349823, 0.51024329662323, 0.49197378754615784, 0.5007163286209106, 0.4937780499458313, 0.5073751211166382, 0.49983206391334534, 0.4977266192436218, 0.4993075132369995, 0.495846152305603, 0.5152333974838257, 0.5085606575012207, 0.5149986147880554, 0.4870666563510895]
,
borderWidth: 1,
radius: 0,
}]
},
options: {
aspectRatio: 2.5,
interaction: {
intersect: false
},
plugins: {
legend: true
},
scales: {
x: {
type: 'linear',
ticks: {
count: 6
},
display: true,
title: {
display: true,
text: 'Epoch'
}
},
y: {
type: 'linear',
display: true,
title: {
display: true,
text: 'Loss'
}
}
}
},
{{< /chart >}}

The model has converged to an acceptable minimum.

## Real-time Interface

A remote-controlled car is programmed using an [ESP 32](https://www.espressif.com/en/products/socs/esp32) to receive real-time commands from a server via Wi-Fi. Communication between the server and the ESP32 is established using Python's [socket](https://docs.python.org/3/library/socket.html) library. 

The server continuously collects 256 EMG data points from a user equipped with an EMG sensor. This data is then passed to the model for inference. The server transmits the control signals to the ESP32 based on the predicted gesture, enabling responsive car movements. The car moves forward when the model predicts an closed fist, and stops when it predicts a open fist. The process operates in a continuous loop until the connection is terminated.

{{< mermaid >}}
graph LR
A[/EMG Sensor + Arduino/] -->|EMG Signal| B{Model}
B -->|Prediction| C(Server)
C -->|Control Signal| D(RC Car)
D -->A

{{< /mermaid >}}

## Conclusion

This project has been an invaluable learning experience, providing a solid foundation in integrating machine learning with real-time applications. Looking ahead, there is significant scope for further development. I plan to expand the dataset by incorporating a wider range of gestures, experiment with advanced model architectures, and explore transfer learning techniques to enhance model performance and adaptability.

The entire codebase for this project can be found on [GitHub](https://github.com/nveshaan/muscle-wave-classifier).