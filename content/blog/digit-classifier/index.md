---
title: "Coding a Neural Network from scratch"
date: 2024-07-21
draft: false
tags: ["machine learning"]
---

## Overview

In this post, I’ll demonstrate how to build a neural network from scratch without relying on popular ML frameworks like PyTorch, TensorFlow, or Keras. Instead, I’ll use Python libraries such as `numpy`, `pandas`, and `matplotlib` to develop a model that classifies handwritten digits.

### Why implement a Neural Net from scratch?

Plenty of ML frameworks offer out-of-the-box functionality for building neural networks. However, implementing one from scratch is a valuable exercise. It helps you understand how neural networks work under the hood, which is essential for designing effective models.

{{< alert icon="circle-info">}}
For a deep dive into neural networks, check out 3Blue1Brown's [series](https://www.3blue1brown.com/topics/neural-networks). Here, I'll focus on the practical implementation.
{{< /alert >}}

## Architecture

The model will consist of an input layer of 784 neurons, two hidden layers with 16 neurons each and an output layer with 10 neurons. This is a very simple configuration relative to modern standards.

Both hidden layers use **sigmoid** as the activation function. The final layer goes through a **softmax** function.

The model uses **batch gradient descent** algorithm to find the minima of cost function.

## Implementation

### Setup

As mentioned above, I will import the required Python libraries.

```py
import numpy as np
import pandas as pd
import matplotlib as plt
```

Then, I will split the [data](https://drive.google.com/file/d/1UcrPb8EZ6WFqm3xZV9saaWZnA6-doQPU/view?usp=sharing) into `train` and `test` sets, taking `m = 30000` as number of training examples.

```py
data = pd.read_csv('/data.csv')
train = data[:30000]
test = data[30000:]

X = train.drop(columns=['label']).transpose() # input
Y = train['label'] # output

X_t = test.drop(columns=['label']).transpose() # input
Y_t = test['label'] # output
```

Now, I will **one-hot encode** the labels

```py
Y_one = np.zeros((m, 10))

for i in range(m):
    Y_one[i][Y[i]] = 1

Y_one = Y_one.T
```

and initialise weights and biases. `np.random.rand()` generates random values in [0, 1], I'll subtract them with `0.5` for better performance

```py
W1 = np.random.rand(16, 784) - 0.5  # 16x784 matrix
B1 = np.random.rand(16) - 0.5       # 16x1 matrix

W2 = np.random.rand(16, 16) - 0.5   # 16x16 matrix
B2 = np.random.rand(16) - 0.5       # 16x1 matrix

W3 = np.random.rand(10, 16) - 0.5   # 10X16 matrix
B3 = np.random.rand(10) - 0.5       # 10x1 matrix
```

At first, these parameters are just random numbers. When used, they produce garbage results. As the model learns to predict the correct values, it tunes them to reasonable numbers. Which, when used, will yield good results.

<!-- And lastly, the hyperparameters.

```py
epoch = 500  # no. of iterations
alpha = 0.8  # learning rate
m = 30000    # no. of training examples
``` -->

### Training

Before going into the training process, I will discuss each part separately.

#### Forward Prop

I will feed the training examples to the input layers, multiplying them by the weights and adding the bias values. This output is then input into the first hidden layer, and this process continues to the final output layer.

```py
Z1 = np.dot(W1, X[i]) + B1
A1 = 1/(1+np.exp(-Z1+1e-5)) # sigmoid

Z2 = np.dot(W2, A1) + B2
A2 = 1/(1+np.exp(-Z2+1e-5)) # sigmoid

Z3 = np.dot(W3, A2) + B3
y = np.exp(Z3+1e-5)
y /= sum(y) # softmax

# I added a small value of 10^-5 to prevent the exponent function from vanishing
```

#### Back Prop

By applying the chain rule of calculus, I will compute the partial derivatives for each node in the layers. However, `numpy` makes implementing backpropagation much simpler.

```py
dz3 = y - Y_one.T[i]
dw3 += np.outer(dz3, A2)
db3 += dz3

da2 = np.dot(W3.T, dz3)
dz2 = A2*(1 - A2)*da2
dw2 += np.outer(dz2, A1)
db2 += dz2

da1 = np.dot(W2.T, dz2)
dz1 = A1*(1 - A1)*da1
dw1 += np.outer(dz1, X[i])
db1 += dz1
```

I'm summing all the increments and decrements to the weights and biases across all training examples and updating them at the end of the epoch.

#### Update Params

I'll adjust the parameters once I've gone through all the training examples.

```py
W3 -= alpha*dw3/m
B3 -= alpha*db3/m
W2 -= alpha*dw2/m
B2 -= alpha*db2/m
W1 -= alpha*dw1/m
B1 -= alpha*db1/m
```

I will repeat the process through multiple iterations (or epochs) to find the minimum value.

### Putting it all together

```py
for run in range(epoch):

    dw1 = np.zeros((16, 784))
    db1 = np.zeros(16)
    dw2 = np.zeros((16, 16))
    db2 = np.zeros(16)
    dw3 = np.zeros((10, 16))
    db3 = np.zeros(10)

    for i in range(m):
        # Forward prop
        Z1 = np.dot(W1, X[i]) + B1
        A1 = 1/(1+np.exp(-Z1+1e-5))

        Z2 = np.dot(W2, A1) + B2
        A2 = 1/(1+np.exp(-Z2+1e-5))

        Z3 = np.dot(W3, A2) + B3
        y = np.exp(Z3+1e-5)
        y /= sum(y)

        # Back prop
        dz3 = y - Y_one.T[i]
        dw3 += np.outer(dz3, A2)
        db3 += dz3
        da2 = np.dot(W3.T, dz3)
        dz2 = A2*(1 - A2)*da2
        dw2 += np.outer(dz2, A1)
        db2 += dz2
        da1 = np.dot(W2.T, dz2)
        dz1 = A1*(1 - A1)*da1
        dw1 += np.outer(dz1, X[i])
        db1 += dz1

    # Update params
    W3 -= alpha*dw3/m
    B3 -= alpha*db3/m
    W2 -= alpha*dw2/m
    B2 -= alpha*db2/m
    W1 -= alpha*dw1/m
    B1 -= alpha*db1/m
```

## Results

I obtained the following results after training the model for `epoch = 500` with a learning rate `alpha = 0.8`.

**Training accuracy:** 87.22 %\
**Test accuracy:** 85.65 %

The model generalises the test data very well. The `cost v/s epoch` plot shows that the cost function converged to a value.

{{< chart >}}
type: 'line',
data: {
labels: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500]
,
datasets: [{
label: 'Cost v/s Epoch',
data: [2.5065865262814757, 2.3539751930996755, 2.3127779043894123, 2.2948839035733233, 2.285534548616548, 2.2787875755075477, 2.273021150982252, 2.2672557693857214, 2.261499479346584, 2.256056779145798, 2.2501483903200987, 2.244191471210831, 2.2381721473258556, 2.232184423383912, 2.2259828198456884, 2.2194309134065504, 2.2129224735398787, 2.206197648855626, 2.1989395485740624, 2.1917472528792574, 2.1841019181004895, 2.1762085994242217, 2.1680529023494266, 2.159495013138281, 2.1510401635532914, 2.1422674860036115, 2.133069262360879, 2.1233819789257495, 2.113685440550318, 2.103302893369122, 2.0928321104407934, 2.081987045334639, 2.0709120237126495, 2.0596694852172055, 2.047942566735026, 2.035531326543498, 2.0228802198758826, 2.0097672505691846, 1.996370379687435, 1.9830442013856076, 1.9698389576577122, 1.9556092265758172, 1.940705696050588, 1.9260385602456356, 1.9113524129668018, 1.896950056005362, 1.8823267286316419, 1.8672291349399723, 1.8518663515530165, 1.8358263259015057, 1.8207240473158097, 1.8058111350929724, 1.7895122390836786, 1.7740953000903972, 1.7592463762163697, 1.7434998756071325, 1.7282462839399868, 1.7129455888942082, 1.697870814569705, 1.6821892041733224, 1.6672534362199274, 1.6522440151084563, 1.6382601452583847, 1.6239599474044568, 1.6093750644935896, 1.596121149877006, 1.5814096681302547, 1.5665918642951366, 1.5542154816232436, 1.538944911732399, 1.526131657967618, 1.5121623705359533, 1.4982951421859876, 1.4861823602209905, 1.4739772910120745, 1.462448760302252, 1.4493875334897532, 1.4369327892423156, 1.425004314268046, 1.413985911508984, 1.4022226044092356, 1.3902604639278238, 1.3792013781140247, 1.3686164405575962, 1.358391135005983, 1.3461352876655364, 1.3354420474523803, 1.325002000056776, 1.315267747404995, 1.304689482585775, 1.2934799124329917, 1.284601858879916, 1.2769443776537504, 1.2642803702744672, 1.2550137507089139, 1.247661835323459, 1.2387415636080015, 1.229354467897958, 1.220447081064787, 1.211515705837055, 1.2042155517698996, 1.1958992414028524, 1.1868753897570956, 1.177888490109728, 1.168711303789655, 1.160255787122157, 1.1530565771248646, 1.1468332248944642, 1.1390263060589534, 1.1298174468758178, 1.1222406040836579, 1.1149385570136694, 1.1077267531310226, 1.0982907253838334, 1.0929917166372756, 1.0842797097217256, 1.079316111223963, 1.070983223866678, 1.0652065901399508, 1.0569818051811928, 1.0521551611368072, 1.0447316308164307, 1.038732044215322, 1.032281940651885, 1.0241970482214895, 1.017543096628427, 1.0126498003343192, 1.0059359033827433, 1.0027435832136131, 0.9953701029087253, 0.9904006619694943, 0.9829923779512129, 0.9809321092592903, 0.9741312387776501, 0.9693183962965884, 0.9645349127599898, 0.9592798901009297, 0.9536161590211499, 0.9488567667639445, 0.9447779411190278, 0.9385584508246423, 0.9338878998263478, 0.9280739816889826, 0.9268199170752489, 0.9205122603720818, 0.9136977239153218, 0.9103191958803943, 0.906647120599332, 0.90627056537782, 0.8981546065079966, 0.8961014775848504, 0.8874406136266683, 0.8847017296246128, 0.8816727770128286, 0.878244898620402, 0.8754127717272857, 0.8702673296804379, 0.867907930909364, 0.8604198216858868, 0.8580663089514361, 0.8588287983148314, 0.8502706300895978, 0.8482732796370446, 0.8434834354421628, 0.8363653887425567, 0.8332946827139134, 0.8312832368416223, 0.8305599408359182, 0.825389683834805, 0.8205077624521903, 0.8186630945019483, 0.8146994900439479, 0.809187651040128, 0.8077890492796431, 0.8059854152340971, 0.8048042155169405, 0.7993371581126769, 0.7984919176290327, 0.7963774106548537, 0.7910137510172301, 0.7868298666089626, 0.7841649683855564, 0.785287863728924, 0.7803570518551755, 0.7773474638148377, 0.7719436603058201, 0.7722900309265552, 0.7670042534957724, 0.7655485771639233, 0.760987091023052, 0.7598852955340495, 0.7597571783017687, 0.754583372247671, 0.7511691972626404, 0.7524508229153437, 0.7513165018438082, 0.7459703936334818, 0.7416156799024164, 0.7403848829578188, 0.7368848203557397, 0.7389708766355844, 0.7360472700733547, 0.7339011403390822, 0.7292715491925991, 0.7258528589240116, 0.7277902058512533, 0.7250484646944753, 0.72024802483005, 0.7189504001368149, 0.7173629740547087, 0.7153785206718689, 0.7173298106818696, 0.7126367795556461, 0.712567690376646, 0.7095555737889976, 0.7048361969572386, 0.7014592437545075, 0.6980432496287868, 0.6970229081170441, 0.704092372585689, 0.6982956950327183, 0.6933289509997201, 0.6932969936958627, 0.6911155049315589, 0.6851808719451946, 0.6866014157417455, 0.682118135971822, 0.6840475085745904, 0.6791470465076282, 0.6761193325425752, 0.6759000241406283, 0.6774560522281204, 0.6709477969560274, 0.6679456627022059, 0.670915556846507, 0.6698394312055871, 0.6662404947210636, 0.6636762324404185, 0.6620792552780735, 0.6568319780989316, 0.6552066641491517, 0.6545987895953002, 0.653157561097464, 0.6562748767527448, 0.6518607207005427, 0.656004683715764, 0.6500756036992358, 0.6465849415173993, 0.6455775578002699, 0.6433810656785274, 0.6422645283756149, 0.640953105104684, 0.6441246321096398, 0.6360154375072565, 0.632415542967967, 0.6356853206833493, 0.6328047141722271, 0.6326739134260524, 0.6309115329583703, 0.6291795845661704, 0.6216267757598692, 0.623684722844123, 0.6199967831102955, 0.6203968043157745, 0.6244338025195691, 0.6190571719056688, 0.6180227002839314, 0.6139255252495831, 0.6132820445367927, 0.6150959171235152, 0.6140610908627933, 0.6088146306836181, 0.6128301036951647, 0.6048616834178199, 0.6039684320873638, 0.6137792900571852, 0.6092420220249573, 0.602592787695053, 0.6036827249850336, 0.5987433186789638, 0.5983181126799283, 0.5975969085901299, 0.5974227385874133, 0.5966566696132044, 0.6040730861390928, 0.5968227338113742, 0.5935698763183577, 0.5952587632436032, 0.5910711784440106, 0.5863782167009186, 0.5886065463014981, 0.5863705710706111, 0.5859003240334825, 0.5817387196543031, 0.5802741148563436, 0.5807582606971478, 0.5766629018715094, 0.5799568493565528, 0.5854453679491486, 0.5782224081458438, 0.5765267700951698, 0.580633144563961, 0.5709544621568362, 0.5682596805807907, 0.5695058800192809, 0.570895444506921, 0.566505440905415, 0.5692324585677266, 0.5646446960143326, 0.5625561883524474, 0.5663374135604107, 0.5691282976087086, 0.5700598329663817, 0.5600875841529928, 0.5658317836719489, 0.5624318362239057, 0.5628628240349377, 0.5556625897269736, 0.5576618250547206, 0.5574362540860935, 0.5615888738328145, 0.5538282916462005, 0.5579679716378265, 0.5554601120982466, 0.5535815570965701, 0.5473992262845875, 0.5484427195513408, 0.5486982903822916, 0.5506240606625807, 0.5508045962330933, 0.5501844542184114, 0.5448073758254823, 0.5456681541196168, 0.544161954650145, 0.5435278037371803, 0.5417191697382108, 0.5423990723435227, 0.538278025682074, 0.5395006825387403, 0.5425787105415563, 0.5361182514508436, 0.5390397905895143, 0.5335538870283995, 0.542193122616206, 0.5361846637175062, 0.5410341909659827, 0.5325046043936972, 0.5329804329782537, 0.5311730563994828, 0.5320481878691934, 0.5270261213689879, 0.5256635824907769, 0.5293871378744597, 0.5274116177254176, 0.5287531463612223, 0.5277208471490804, 0.5256852542250219, 0.5258233811453631, 0.5231161352646991, 0.5223251809767825, 0.5190817761141042, 0.520675434047943, 0.5176408112768428, 0.5151231319740605, 0.5233876992852503, 0.5224865126913514, 0.517727177081563, 0.5190562271408679, 0.5187285162277265, 0.5149147805879256, 0.5181817185515198, 0.5163082541731672, 0.5108580309093099, 0.5164893877866606, 0.5091738163256955, 0.5092346292102197, 0.511489559309754, 0.5122631310339993, 0.5156864673056173, 0.5092684560694732, 0.518648878690226, 0.5153072858258735, 0.5055972161623078, 0.5064462124365287, 0.5058703235700432, 0.5000535520725359, 0.5106081949526337, 0.5093730478437448, 0.5042363878236464, 0.5048770534403317, 0.5004625139020104, 0.5003185082600495, 0.4975389431714065, 0.50519355306217, 0.5003290634323909, 0.4974876339675489, 0.49942850185415644, 0.5014820994607367, 0.497696288083882, 0.4933804628703345, 0.4946308814376837, 0.4923912525779405, 0.4940090226570394, 0.49015017598894267, 0.48828325151117064, 0.491163351693653, 0.49151980578840654, 0.4873457777371996, 0.49117119391844954, 0.4861376368323927, 0.4875978024762181, 0.48883817194454415, 0.48674997862325003, 0.48986754744800615, 0.4897430119608027, 0.4879556678056332, 0.4849332526906925, 0.48492159231965887, 0.4789246794361896, 0.4804692460409831, 0.48306681321254213, 0.4832885859803546, 0.4893042235947236, 0.48212123103915694, 0.4839741977808423, 0.47749233800354546, 0.48327087346038805, 0.48141172844485414, 0.4738848044196047, 0.4789186038298284, 0.4741063099967479, 0.47424508591721176, 0.47507289733429525, 0.47242550046794557, 0.4760285294486967, 0.47124816331396424, 0.4716704573258757, 0.46866951192464923, 0.4699682150337284, 0.4797567364874804, 0.4715918895467555, 0.4691211524350459, 0.47401229854342036, 0.46540242749299227, 0.4657155297471419, 0.4664533268294183, 0.47345298590550927, 0.48047037233819717, 0.46553322539773584, 0.463961233432475, 0.47221128162080084, 0.46185383412323244, 0.46137763596493314, 0.4615613870178719, 0.46710919787011285, 0.4618741586208516, 0.4645984223591501, 0.4650607744662762, 0.4607723631108513, 0.4583903774815227, 0.46252221036612295, 0.45571559741138573, 0.45738348677842666, 0.45895884030724815, 0.46012539712602607, 0.45497003824409216, 0.46676481953395027, 0.45602043166408546, 0.45424086109724915, 0.45000743230178397, 0.4467801116357947, 0.4584968500680742, 0.45370585113791806, 0.45028627934870247, 0.44990794787837934, 0.44805635434570695, 0.44642123383254034, 0.44742362025462146, 0.4486323783245211, 0.4434788906040156, 0.4411876084928964, 0.4462975465186776, 0.44276575500105403, 0.44730942624297876, 0.44201248375314794, 0.44375339163868094, 0.44305335091823683, 0.45602313185564586, 0.44039601844311654, 0.4385077616354422, 0.44951002778386706, 0.4464869698611904, 0.44048760705495954, 0.4382084567329315, 0.4380149495425529, 0.4360931897238319, 0.4399375213496081, 0.4409107939143895, 0.4336378840552746, 0.4389841894774701]
,
borderWidth: 1,
radius: 0,
}]
},
options: {
aspectRatio: 1.5,
interaction: {
intersect: false
},
plugins: {
legend: false
},
scales: {
x: {
type: 'linear',
ticks: {
count: 6
},
display: true,
title: {
display: true,
text: 'Epoch'
}
},
y: {
type: 'linear',
display: true,
title: {
display: true,
text: 'Cost'
}
}
}
},
{{< /chart >}}
\
I executed the code in Google Colab. The link to that notebook is [here](https://colab.research.google.com/drive/15LAq8kzqS7dQm2l6mmlC4Iik0wl-ANUX?usp=sharing).
